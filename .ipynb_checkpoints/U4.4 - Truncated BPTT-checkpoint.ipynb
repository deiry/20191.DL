{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated BPTT\n",
    "\n",
    "When the sequences are very long (thousands of points), the network training can be very slow and the memory requirements increase. The truncated BPTT is an alternative similar to mini-batch training in Dense Networks, even though in RNN the batch parameter can also be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./Images/rnn_tbptt_2.png \"Truncated BPTT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TBPTT can be implemented by setting up the data appropriately. Let's remember that for recurrent neural networks, data must have the format **[n_samples,n_times,n_features]**, so if you want to use Truncated BPTT you just have to split the sequences into more **n_samples** of less **n_times**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BUT**, **Is it possible that the LSTM may find dependencies between the sequences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it’s not possible unless you go for the stateful LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the use of Truncated BPTT requires to set up the **Stateful** mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Extracted from: http://philipperemy.github.io/keras-stateful-lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also a problem of classifying sequences. The data matrix $X$ is made exclusively of zeros except in the first column where exactly half of the values are 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "N_train = 1000\n",
    "X_train = np.zeros((N_train,20))\n",
    "from numpy.random import choice\n",
    "one_indexes = choice(a=N_train, size=N_train / 2, replace=False)\n",
    "X_train[one_indexes, 0] = 1  # very long term memory.\n",
    "#--------------------------------\n",
    "N_test = 200\n",
    "X_test = np.zeros((N_test,20))\n",
    "from numpy.random import choice\n",
    "one_indexes = choice(a=N_test, size=N_test / 2, replace=False)\n",
    "X_test[one_indexes, 0] = 1  # very long term memory.\n",
    "print(X_train[:10,:2])\n",
    "print(X_test[:10,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(x_train, y_train, window_length):\n",
    "    windows = []\n",
    "    windows_y = []\n",
    "    for i, sequence in enumerate(x_train):\n",
    "        len_seq = len(sequence)\n",
    "        for window_start in range(0, len_seq - window_length + 1):\n",
    "            window_end = window_start + window_length\n",
    "            window = sequence[window_start:window_end]\n",
    "            windows.append(window)\n",
    "            windows_y.append(y_train[i])\n",
    "    return np.array(windows), np.array(windows_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11000, 10, 1)\n",
      "(11000, 1)\n",
      "(2200, 10, 1)\n",
      "(2200, 1)\n"
     ]
    }
   ],
   "source": [
    "#Split the sequences into two sequences of length 10\n",
    "window_length = 10\n",
    "\n",
    "x_train, y_train = prepare_sequences(X_train, X_train[:,0], window_length)\n",
    "x_test, y_test = prepare_sequences(X_test, X_test[:,0], window_length)\n",
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed in a sequence of length 20, we have exactly (20−10+1) sequences of length 10. Because we have 1000 samples (sequences) in the training set, we have 1000∗11=11000 subsequences in our resulting training set ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a regular LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the original sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6858 - acc: 0.5180 - val_loss: 0.5868 - val_acc: 0.5000\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2867 - acc: 0.9430 - val_loss: 0.1594 - val_acc: 1.0000\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1098 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0565 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0336 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0119 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0093 - val_acc: 1.0000\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 1.0000\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 11\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(20, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train.reshape(1000,20,1), X_train[:,0].reshape(1000,1), batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test.reshape(200,20,1), X_test[:,0].reshape(200,1)), shuffle=False)\n",
    "score, acc = model.evaluate(X_test.reshape(200,20,1),X_test[:,0].reshape(200,1), batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the splitted sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building STATELESS model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/lib/python2.7/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples, validate on 2200 samples\n",
      "Epoch 1/15\n",
      "11000/11000 [==============================] - 8s 715us/step - loss: 0.6705 - acc: 0.5199 - val_loss: 0.6616 - val_acc: 0.5455\n",
      "Epoch 2/15\n",
      "11000/11000 [==============================] - 7s 652us/step - loss: 0.6625 - acc: 0.5378 - val_loss: 0.6612 - val_acc: 0.5455\n",
      "Epoch 3/15\n",
      "11000/11000 [==============================] - 7s 646us/step - loss: 0.6621 - acc: 0.5443 - val_loss: 0.6611 - val_acc: 0.5455\n",
      "Epoch 4/15\n",
      "11000/11000 [==============================] - 7s 644us/step - loss: 0.6619 - acc: 0.5455 - val_loss: 0.6610 - val_acc: 0.5455\n",
      "Epoch 5/15\n",
      "11000/11000 [==============================] - 7s 665us/step - loss: 0.6618 - acc: 0.5455 - val_loss: 0.6609 - val_acc: 0.5455\n",
      "Epoch 6/15\n",
      "11000/11000 [==============================] - 7s 669us/step - loss: 0.6617 - acc: 0.5455 - val_loss: 0.6609 - val_acc: 0.5455\n",
      "Epoch 7/15\n",
      "11000/11000 [==============================] - 7s 669us/step - loss: 0.6616 - acc: 0.5455 - val_loss: 0.6608 - val_acc: 0.5455\n",
      "Epoch 8/15\n",
      "11000/11000 [==============================] - 7s 668us/step - loss: 0.6616 - acc: 0.5455 - val_loss: 0.6608 - val_acc: 0.5455\n",
      "Epoch 9/15\n",
      "11000/11000 [==============================] - 7s 669us/step - loss: 0.6615 - acc: 0.5455 - val_loss: 0.6608 - val_acc: 0.5455\n",
      "Epoch 10/15\n",
      "11000/11000 [==============================] - 7s 672us/step - loss: 0.6615 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n",
      "Epoch 11/15\n",
      "11000/11000 [==============================] - 7s 663us/step - loss: 0.6614 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n",
      "Epoch 12/15\n",
      "11000/11000 [==============================] - 7s 665us/step - loss: 0.6614 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n",
      "Epoch 13/15\n",
      "11000/11000 [==============================] - 7s 660us/step - loss: 0.6613 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n",
      "Epoch 14/15\n",
      "11000/11000 [==============================] - 7s 642us/step - loss: 0.6613 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n",
      "Epoch 15/15\n",
      "11000/11000 [==============================] - 7s 656us/step - loss: 0.6613 - acc: 0.5455 - val_loss: 0.6607 - val_acc: 0.5455\n"
     ]
    }
   ],
   "source": [
    "print('Building STATELESS model...')\n",
    "max_len = 10\n",
    "batch_size = 11\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(max_len, 1), return_sequences=False, stateful=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(x_test, y_test), shuffle=False)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545468091965"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
