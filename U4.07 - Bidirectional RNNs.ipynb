{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import libraries.project_tests as tests\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some problems the information required to make a prediction in one point of a sequence, includes not only pass information but also \"future\" information, i.e., information before and after of the target point in the sequence. This also imply that such informaction must be available to make the perdictions. For instance, in translation problems usually you need to know an entire sentence beforhand in order to translate it correctly.   The bidirectional RNNs are a modification of the standard RNNs that incorporate additional layers which transmit the information from a the time $t+1$ to the time $t$. The forward and backward layers do not have conextion among them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./Images/RNN_arc_3.png \"Neuronas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on the Machine Translation material included in the Deep Learning Specilization offered by Coursera: https://es.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model architecture could be used for language translation, however it would require hundred of thousands of texts, a big computational power (GPU) and hundreds of hours in order to get a fairly accurate model. Therefore in this example, we are going to build a Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). \n",
    "\n",
    "The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) and translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = tuple(open('data/small_vocab_en', 'r'))\n",
    "french_sentences = tuple(open('data/small_vocab_fr', 'r'))\n",
    "english_sentences = english_sentences[:1000]\n",
    "french_sentences = french_sentences[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13205 English words.\n",
      "194 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"but\" \"the\" \"and\" \"never\"\n",
      "\n",
      "14174 French words.\n",
      "262 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"jamais\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying padding to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 19\n",
      "English vocabulary size: 171\n",
      "French vocabulary size: 256\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the actual French trnaslation, it is necessary to define a function to decode the network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 19, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_french_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_french_sentencesOH = np.zeros((preproc_french_sentences.shape[0],preproc_french_sentences.shape[1],french_vocab_size))\n",
    "for i in range(preproc_french_sentences.shape[0]):\n",
    "    preproc_french_sentencesOH[i,:,:] = np_utils.to_categorical(preproc_french_sentences[i,:,0],french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 19, 256)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_french_sentencesOH.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 5.4247 - val_loss: 5.2637\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 284us/step - loss: 5.0933 - val_loss: 4.8736\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 295us/step - loss: 4.5909 - val_loss: 4.2003\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 295us/step - loss: 3.8151 - val_loss: 3.4351\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 286us/step - loss: 3.3260 - val_loss: 3.2082\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 290us/step - loss: 3.1813 - val_loss: 3.1024\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 287us/step - loss: 3.0799 - val_loss: 3.0244\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 278us/step - loss: 3.0096 - val_loss: 2.9684\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 285us/step - loss: 2.9565 - val_loss: 2.9256\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 295us/step - loss: 2.9169 - val_loss: 2.8931\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 392us/step - loss: 2.8844 - val_loss: 2.8669\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 286us/step - loss: 2.8558 - val_loss: 2.8420\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 292us/step - loss: 2.8301 - val_loss: 2.8202\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 276us/step - loss: 2.8071 - val_loss: 2.8001\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 304us/step - loss: 2.7856 - val_loss: 2.7812\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 293us/step - loss: 2.7651 - val_loss: 2.7626\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 287us/step - loss: 2.7453 - val_loss: 2.7462\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 311us/step - loss: 2.7266 - val_loss: 2.7296\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 388us/step - loss: 2.7082 - val_loss: 2.7136\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 295us/step - loss: 2.6897 - val_loss: 2.6977\n",
      "les est est est est est est est est <PAD> est <PAD> est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate))\n",
    "    \n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=100, epochs=20, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 19, 1)             0         \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 19, 64)            12672     \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, 19, 256)           16640     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 19, 256)           0         \n",
      "=================================================================\n",
      "Total params: 29,312\n",
      "Trainable params: 29,312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.8481039e-03, 2.9907644e-02, 1.9580347e-02, ..., 4.8219142e-04,\n",
       "        4.1300952e-04, 2.9765230e-04],\n",
       "       [7.3605524e-03, 5.2269921e-02, 2.3590535e-02, ..., 2.6736938e-04,\n",
       "        1.8885369e-04, 1.3384754e-04],\n",
       "       [3.6405202e-02, 1.4590807e-01, 4.4361684e-02, ..., 1.7853203e-04,\n",
       "        1.2387775e-04, 9.7096687e-05],\n",
       "       ...,\n",
       "       [9.3317574e-01, 6.6529643e-03, 3.3371458e-03, ..., 4.1748288e-05,\n",
       "        3.1887499e-05, 1.5248005e-05],\n",
       "       [9.3938023e-01, 5.6127198e-03, 2.8847037e-03, ..., 4.1820680e-05,\n",
       "        3.2819607e-05, 1.5506728e-05],\n",
       "       [9.4125164e-01, 5.2856361e-03, 2.7430281e-03, ..., 4.1940977e-05,\n",
       "        3.3422035e-05, 1.5756106e-05]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn_model.predict(tmp_x[:1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
